{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated spectrograms for conditions: ['p558_p547_p558_001.npy_R', 'p558_p547_p558_001.npy_F', 'p558_p547_p558_001.npy_U', 'p558_p547_p558_001.npy_FU', 'p558_p547_p558_001.npy_RF', 'p558_p547_p558_001.npy_RU', 'p558_p547_p558_001.npy_RFU']\n"
     ]
    }
   ],
   "source": [
    "# # demo_conversion.py\n",
    "# import torch\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# from hparams import hparams\n",
    "# from utils import pad_seq_to_2, quantize_f0_numpy\n",
    "# from model import Generator_3 as Generator\n",
    "# from model import Generator_6 as F0_Converter\n",
    "\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # -------------------------\n",
    "# # Load Models & Checkpoints\n",
    "# # -------------------------\n",
    "# G = Generator(hparams).eval().to(device)\n",
    "# g_checkpoint = torch.load('assets/100000-G.ckpt', map_location=lambda storage, loc: storage)\n",
    "# G.load_state_dict(g_checkpoint['model'], strict=False)\n",
    "\n",
    "# P = F0_Converter(hparams).eval().to(device)\n",
    "# p_checkpoint = torch.load('assets/100000-P.ckpt', map_location=lambda storage, loc: storage)\n",
    "# P.load_state_dict(p_checkpoint['model'])\n",
    "\n",
    "# # -------------------------\n",
    "# # Load Demo Metadata\n",
    "# # -------------------------\n",
    "# metadata = pickle.load(open('assets/demo.pkl', \"rb\"))\n",
    "\n",
    "# # -------------------------\n",
    "# # Process Source Utterance (sbmt_i)\n",
    "# # -------------------------\n",
    "# sbmt_i = metadata[0]\n",
    "# # Load source speaker embedding and ensure it is 2D: [batch, d_emb]\n",
    "# emb_org = torch.from_numpy(sbmt_i[1]).to(device)\n",
    "# if emb_org.dim() == 1:\n",
    "#     emb_org = emb_org.unsqueeze(0)\n",
    "\n",
    "# x_org, f0_org, len_org, uid_org = sbmt_i[2]\n",
    "# uttr_org_pad, _ = pad_seq_to_2(x_org[np.newaxis, :, :], 192)\n",
    "# uttr_org_pad = torch.from_numpy(uttr_org_pad).to(device)\n",
    "\n",
    "# # Process F0 for source: if raw (1D), pad & quantize; otherwise, adjust dimensions.\n",
    "# if f0_org.ndim == 1:\n",
    "#     f0_org_pad = np.pad(f0_org, (0, 192 - len_org), 'constant', constant_values=(0, 0))\n",
    "#     f0_org_quantized = quantize_f0_numpy(f0_org_pad)[0]\n",
    "# else:\n",
    "#     if f0_org.shape[0] < 192:\n",
    "#         f0_org_quantized = np.pad(f0_org, ((0, 192 - f0_org.shape[0]), (0, 0)),\n",
    "#                                    'constant', constant_values=(0, 0))\n",
    "#     else:\n",
    "#         f0_org_quantized = f0_org[:192, :]\n",
    "# f0_org_onehot = f0_org_quantized[np.newaxis, :, :]\n",
    "# f0_org_onehot = torch.from_numpy(f0_org_onehot).to(device)\n",
    "# uttr_f0_org = torch.cat((uttr_org_pad, f0_org_onehot), dim=-1)\n",
    "\n",
    "# # -------------------------\n",
    "# # Process Target Utterance (sbmt_j)\n",
    "# # -------------------------\n",
    "# sbmt_j = metadata[1]\n",
    "# # Load target speaker embedding and ensure it is 2D: [batch, d_emb]\n",
    "# emb_trg = torch.from_numpy(sbmt_j[1]).to(device)\n",
    "# if emb_trg.dim() == 1:\n",
    "#     emb_trg = emb_trg.unsqueeze(0)\n",
    "\n",
    "# x_trg, f0_trg, len_trg, uid_trg = sbmt_j[2]\n",
    "# uttr_trg_pad, _ = pad_seq_to_2(x_trg[np.newaxis, :, :], 192)\n",
    "# uttr_trg_pad = torch.from_numpy(uttr_trg_pad).to(device)\n",
    "\n",
    "# if f0_trg.ndim == 1:\n",
    "#     f0_trg_pad = np.pad(f0_trg, (0, 192 - len_trg), 'constant', constant_values=(0, 0))\n",
    "#     f0_trg_quantized = quantize_f0_numpy(f0_trg_pad)[0]\n",
    "# else:\n",
    "#     if f0_trg.shape[0] < 192:\n",
    "#         f0_trg_quantized = np.pad(f0_trg, ((0, 192 - f0_trg.shape[0]), (0, 0)),\n",
    "#                                    'constant', constant_values=(0, 0))\n",
    "#     else:\n",
    "#         f0_trg_quantized = f0_trg[:192, :]\n",
    "# f0_trg_onehot = f0_trg_quantized[np.newaxis, :, :]\n",
    "# f0_trg_onehot = torch.from_numpy(f0_trg_onehot).to(device)\n",
    "\n",
    "# # -------------------------\n",
    "# # Run F0 Converter to Get Target F0 Features\n",
    "# # -------------------------\n",
    "# with torch.no_grad():\n",
    "#     f0_pred = P(uttr_org_pad, f0_trg_onehot)[0]\n",
    "#     f0_pred_quantized = f0_pred.argmax(dim=-1).squeeze(0)\n",
    "#     f0_con_onehot = torch.zeros((1, 192, 257), device=device)\n",
    "#     f0_con_onehot[0, torch.arange(192), f0_pred_quantized] = 1\n",
    "# uttr_f0_trg = torch.cat((uttr_org_pad, f0_con_onehot), dim=-1)\n",
    "\n",
    "# # -------------------------\n",
    "# # Run Generator Under Different Conditions\n",
    "# # -------------------------\n",
    "# conditions = ['FU', 'F', 'U', 'R', 'RF', 'RU', 'RFU']\n",
    "# spect_vc = []\n",
    "# with torch.no_grad():\n",
    "#     for condition in conditions:\n",
    "#         if condition == 'R':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_org, uttr_trg_pad, emb_org)\n",
    "#         if condition == 'F':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_trg, uttr_org_pad, emb_org)\n",
    "#         if condition == 'U':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_org, uttr_org_pad, emb_trg)\n",
    "#         if condition == 'RF':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_trg, uttr_trg_pad, emb_org)\n",
    "#         if condition == 'RU':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_org, uttr_trg_pad, emb_trg)\n",
    "#         if condition == 'FU':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_trg, uttr_org_pad, emb_trg)\n",
    "#         if condition == 'RFU':\n",
    "#             x_identic_val, var, mu = G(uttr_f0_trg, uttr_trg_pad, emb_trg)\n",
    "            \n",
    "#         # Choose output length: if condition contains 'R', use len_trg; otherwise, use len_org.\n",
    "#         # if 'R' in condition:\n",
    "#         #     uttr_trg_out = x_identic_val[0, :len_trg, :].cpu().numpy()\n",
    "#         # else:\n",
    "#         #     uttr_trg_out = x_identic_val[0, :len_org, :].cpu().numpy()\n",
    "            \n",
    "#         uttr_trg_out = x_identic_val[0, :len_org, :].cpu().numpy()\n",
    "                \n",
    "#         spect_vc.append( ('{}_{}_{}_{}'.format(sbmt_i[0], sbmt_j[0], uid_org, condition), uttr_trg_out) )\n",
    "\n",
    "# print(\"Generated spectrograms for conditions:\", [cond for cond, _ in spect_vc])\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from hparams import hparams\n",
    "from utils import pad_seq_to_2, quantize_f0_numpy\n",
    "from model import Generator_3 as Generator\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# -------------------------\n",
    "# Load Model & Checkpoint for Generator\n",
    "# -------------------------\n",
    "G = Generator(hparams).eval().to(device)\n",
    "g_checkpoint = torch.load('assets/800000-G-B10.ckpt', map_location=lambda storage, loc: storage)\n",
    "G.load_state_dict(g_checkpoint['model'], strict=False)\n",
    "\n",
    "# -------------------------\n",
    "# Load Demo Metadata\n",
    "# -------------------------\n",
    "metadata = pickle.load(open('assets/demo.pkl', \"rb\"))\n",
    "\n",
    "# -------------------------\n",
    "# Process Source Utterance (sbmt_i)\n",
    "# -------------------------\n",
    "sbmt_i = metadata[0]\n",
    "# Load source speaker embedding and ensure it is 2D: [batch, d_emb]\n",
    "emb_org = torch.from_numpy(sbmt_i[1]).to(device)\n",
    "if emb_org.dim() == 1:\n",
    "    emb_org = emb_org.unsqueeze(0)\n",
    "\n",
    "x_org, f0_org, len_org, uid_org = sbmt_i[2]\n",
    "# Crop x_org to 192 frames if necessary\n",
    "if x_org.shape[0] > 192:\n",
    "    x_org = x_org[:192, :]\n",
    "    len_org = 192\n",
    "# Also crop f0_org to 192 frames if needed (for 1D f0)\n",
    "if f0_org.ndim == 1 and f0_org.shape[0] > 192:\n",
    "    f0_org = f0_org[:192]\n",
    "uttr_org_pad, _ = pad_seq_to_2(x_org[np.newaxis, :, :], 192)\n",
    "uttr_org_pad = torch.from_numpy(uttr_org_pad).to(device)\n",
    "\n",
    "# Process F0 for source: if raw (1D), pad & quantize; otherwise, adjust dimensions.\n",
    "if f0_org.ndim == 1:\n",
    "    f0_org_pad = np.pad(f0_org, (0, 192 - len_org), 'constant', constant_values=(0, 0))\n",
    "    f0_org_quantized = quantize_f0_numpy(f0_org_pad)[0]\n",
    "else:\n",
    "    if f0_org.shape[0] < 192:\n",
    "        f0_org_quantized = np.pad(f0_org, ((0, 192 - f0_org.shape[0]), (0, 0)),\n",
    "                                   'constant', constant_values=(0, 0))\n",
    "    else:\n",
    "        f0_org_quantized = f0_org[:192, :]\n",
    "f0_org_onehot = f0_org_quantized[np.newaxis, :, :]\n",
    "f0_org_onehot = torch.from_numpy(f0_org_onehot).to(device)\n",
    "uttr_f0_org = torch.cat((uttr_org_pad, f0_org_onehot), dim=-1)\n",
    "\n",
    "# -------------------------\n",
    "# Process Target Utterance (sbmt_j)\n",
    "# -------------------------\n",
    "sbmt_j = metadata[1]\n",
    "# Load target speaker embedding and ensure it is 2D: [batch, d_emb]\n",
    "emb_trg = torch.from_numpy(sbmt_j[1]).to(device)\n",
    "if emb_trg.dim() == 1:\n",
    "    emb_trg = emb_trg.unsqueeze(0)\n",
    "\n",
    "x_trg, f0_trg, len_trg, uid_trg = sbmt_j[2]\n",
    "# Crop x_trg to 192 frames if necessary\n",
    "if x_trg.shape[0] > 192:\n",
    "    x_trg = x_trg[:192, :]\n",
    "    len_trg = 192\n",
    "# Also crop f0_trg to 192 frames if needed (for 1D f0)\n",
    "if f0_trg.ndim == 1 and f0_trg.shape[0] > 192:\n",
    "    f0_trg = f0_trg[:192]\n",
    "uttr_trg_pad, _ = pad_seq_to_2(x_trg[np.newaxis, :, :], 192)\n",
    "uttr_trg_pad = torch.from_numpy(uttr_trg_pad).to(device)\n",
    "\n",
    "if f0_trg.ndim == 1:\n",
    "    f0_trg_pad = np.pad(f0_trg, (0, 192 - len_trg), 'constant', constant_values=(0, 0))\n",
    "    f0_trg_quantized = quantize_f0_numpy(f0_trg_pad)[0]\n",
    "else:\n",
    "    if f0_trg.shape[0] < 192:\n",
    "        f0_trg_quantized = np.pad(f0_trg, ((0, 192 - f0_trg.shape[0]), (0, 0)),\n",
    "                                   'constant', constant_values=(0, 0))\n",
    "    else:\n",
    "        f0_trg_quantized = f0_trg[:192, :]\n",
    "f0_trg_onehot = f0_trg_quantized[np.newaxis, :, :]\n",
    "f0_trg_onehot = torch.from_numpy(f0_trg_onehot).to(device)\n",
    "\n",
    "# Instead of using the F0 converter, directly create the target F0 input:\n",
    "uttr_f0_trg = torch.cat((uttr_trg_pad, f0_trg_onehot), dim=-1)\n",
    "\n",
    "# -------------------------\n",
    "# Run Generator Under Different Conditions\n",
    "# -------------------------\n",
    "conditions = ['R', 'F', 'U', 'FU', 'RF', 'RU', 'RFU']\n",
    "spect_vc = []\n",
    "with torch.no_grad():\n",
    "    for condition in conditions:\n",
    "        if condition == 'R':\n",
    "            x_identic_val, var, mu = G(uttr_f0_org, uttr_trg_pad, emb_org)\n",
    "        if condition == 'F':\n",
    "            x_identic_val, var, mu = G(uttr_f0_trg, uttr_org_pad, emb_org)\n",
    "        if condition == 'U':\n",
    "            x_identic_val, var, mu = G(uttr_f0_org, uttr_org_pad, emb_trg)\n",
    "        if condition == 'RF':\n",
    "            x_identic_val, var, mu = G(uttr_f0_trg, uttr_trg_pad, emb_org)\n",
    "        if condition == 'RU':\n",
    "            x_identic_val, var, mu = G(uttr_f0_org, uttr_trg_pad, emb_trg)\n",
    "        if condition == 'FU':\n",
    "            x_identic_val, var, mu = G(uttr_f0_trg, uttr_org_pad, emb_trg)\n",
    "        if condition == 'RFU':\n",
    "            x_identic_val, var, mu = G(uttr_f0_trg, uttr_trg_pad, emb_trg)\n",
    "            \n",
    "        if 'R' in condition:\n",
    "            uttr_trg_out = x_identic_val[0, :len_trg, :].cpu().numpy()\n",
    "        else:\n",
    "            uttr_trg_out = x_identic_val[0, :len_org, :].cpu().numpy()\n",
    "                \n",
    "        spect_vc.append( ('{}_{}_{}_{}'.format(sbmt_i[0], sbmt_j[0], uid_org, condition), uttr_trg_out) )\n",
    "\n",
    "print(\"Generated spectrograms for conditions:\", [cond for cond, _ in spect_vc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 30976/30976 [02:49<00:00, 182.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 34560/34560 [03:09<00:00, 182.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 34560/34560 [03:06<00:00, 185.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_FU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 34560/34560 [03:11<00:00, 180.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 30976/30976 [02:49<00:00, 182.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_RU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 30976/30976 [02:49<00:00, 182.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p558_p547_p558_001.npy_RFU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 30976/30976 [02:49<00:00, 182.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# spectrogram to waveform\n",
    "import torch\n",
    "import soundfile\n",
    "import pickle\n",
    "import os\n",
    "from synthesis import build_model\n",
    "from synthesis import wavegen\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "model = build_model().to(device)\n",
    "checkpoint = torch.load(\"assets/checkpoint_step001000000_ema.pth\", map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "for spect in spect_vc:\n",
    "    name = spect[0]\n",
    "    c = spect[1]\n",
    "    print(name)\n",
    "    waveform = wavegen(model, c=c)   \n",
    "    soundfile.write('results/'+name+'.wav', waveform, samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
